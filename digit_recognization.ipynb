{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/birajsth/MachineLearning-with-tensorflow/blob/master/digit_recognization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xXxGlyq0Iqs",
        "colab_type": "text"
      },
      "source": [
        "## Digit Recognization from scratch with MNIST Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i8LlOjV07YF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xz-5Ic20YSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "  def __init__(self,sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1]), sizes[1:]]\n",
        "  \n",
        "  def feedforward(self, a):\n",
        "    '''Returns output of a network for a given input a i.e. a′= σ(wa+b)'''\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "  \n",
        "  def SGD(self, training_data, epochs, \n",
        "          batch_size, eta, test_data=None):\n",
        "    '''Train the neural network uing mini-batch stochastic\n",
        "    gradient descent.\n",
        "    Args:\n",
        "      training_data: list of tuples of training inputs and labels.\n",
        "      epochs: the number of epochs to train for\n",
        "      batch_size: size of the mini-batches to use when sampling\n",
        "      eta: learning rate\n",
        "      test_data: list of tuples of test inputs and labels.\n",
        "    '''\n",
        "    if test_data: n_test = len(test_data)\n",
        "    n = len(training_data)\n",
        "    for j in xrange(epochs):\n",
        "      random.seed(0)\n",
        "      random.shuffle(training_data)\n",
        "      mini_batches = [training_data[k:k+batch_size] \n",
        "                      for k in range(0, n, batch_size)]\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta)\n",
        "      if test_data:\n",
        "        print(f'Epoch {j}: {self.evaluate(test_data)} / {n_test}')\n",
        "      else:\n",
        "        print(f'Epoch {j} complete')\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "      '''Update the network;s weights and biases by appling \n",
        "      gradient descent using backpropagation to a single mini batch.\n",
        "      Args:\n",
        "        mini_batch: list of tuples (x, y)\n",
        "        eta: learning rate'''\n",
        "      nable_b = [np.zeros(b.shape) for b in self.biases]\n",
        "      nable_w = [np.zeros(w.shape) for w in self.weights]\n",
        "      for x, y in mini_batch:\n",
        "        delta_nable_b, delta_nable_w = self.backprop(x, y)\n",
        "        nable_b = [nb+dnb for nb, dnb in zip(nable_b, delta_nable_b)]\n",
        "        nable_w = [nw+dnw for nw, dnw in zip(nable_w, delta_nable_b)]\n",
        "      self.weights = [w-(eta/len(mini_batch))*nw \n",
        "                     for w, nw in zip(self.weights, nable_w)]\n",
        "      self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                    for n, nb in zip(self.biases, nable_b)]\n",
        "    \n",
        "     def backprop(self,x,y):\n",
        "        '''Return a tuple (nabla_b,nabla_w) representing the gradient for \n",
        "        the cost function C_x. nabla_b and nabla_w are layer by layer list of numpy arrays,\n",
        "        similar to self.biases and self.weights.'''\n",
        "        \n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        # feedforward\n",
        "        activation = w\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "          z = np.dot(w, activation) + b\n",
        "          zs.append(z)\n",
        "          activation = sigmoid(z)\n",
        "          activations.append(activation)\n",
        "        \n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little \n",
        "        # differently to  the notation in Chapter 2 of the book. Here,\n",
        "        # l = 1 means the last layer of neurons, l=2 is the \n",
        "        # second-last layer , and so on. It's a renumbering of the \n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that python can use negative indices in list:\n",
        "        for l in xrange(2, self.num_layers):\n",
        "          z = zs[-1]\n",
        "          sp = sigmoid_prime(z)\n",
        "          delta - np.dot(self.weights[-l+1].transpose(), delta)* sp\n",
        "          nabla_b[-1] = delta\n",
        "          nabla_w[-1] = np.dot(delta, activations[-l+1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "      \n",
        "      def evaluate(self, test_data):\n",
        "        '''Return the number of test inputs for which the neural\n",
        "        network outputs the correct result. Note that the neural \n",
        "        network's output is assumed to be the index of whichever \n",
        "        neruon in the final layer has the highest activation.'''\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "      \n",
        "      def cost_derivative(self, output_activations, y):\n",
        "        '''Return the vector of partial derivativevs /partial C_x/\n",
        "        \\partial a for the output activations.'''\n",
        "        return (output_activations - y)\n",
        "\n",
        "#helper functions\n",
        "def sigmoid(z):\n",
        "  '''The sigmoid function.'''\n",
        "  return 1/(1 + np.exp(z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  '''Derivative of the sigmoid function.'''\n",
        "  return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJXHrk7d0fiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}